{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a python notebook, that is able to run all models that we have made thus far. Truly One For All, no one man should have all this power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import liberaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datamanagement\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import datetime\n",
    "import copy\n",
    "\n",
    "# import models\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# import evaluation\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, roc_curve, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# import visualisation\n",
    "from sklearn.tree import export_graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = pd.read_csv(\"../NOTEBOOKS TO REVIEW/Job/dataset_cyb&non_v2.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = copy.deepcopy(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16564145, 103)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Models functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def now():\n",
    "    now = datetime.datetime.now()   \n",
    "    return now.strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_model(train,test,train_labels,test_labels):\n",
    "    #train model\n",
    "    clf = svm.SVC(kernel='linear')\n",
    "    clf.fit(train, train_labels)\n",
    "    \n",
    "    #test model\n",
    "    y_pred = clf.predict(test)\n",
    "    score = classification_report(y_pred, test_labels, output_dict=True)\n",
    "    df_score = pd.DataFrame(score).transpose()\n",
    "    df_score.to_csv('svm_score_final_run_'+now()+'.csv', sep=\";\", index=False)\n",
    "    \n",
    "    #saving TP\n",
    "    SVMTest = test.copy()\n",
    "    SVMTest['Predicted'] = y_pred\n",
    "    SVMTest['Actual'] = test_labels\n",
    "    SVMTest = SVMTest.loc[SVMTest['Predicted'] == SVMTest['Actual']]\n",
    "    SVM_Predictions = data.iloc[list(SVMTest.index.values.tolist())]\n",
    "    SVM_Predictions.to_csv('svm_TP_final_run_'+now()+'.csv', sep=\";\", index=False)\n",
    "      \n",
    "    #postitive target feature importance\n",
    "    svm_feature_result = pd.DataFrame({'feature': train.columns,'importance': clf.coef_[0]}).sort_values('importance', ascending = False)\n",
    "    svm_feature_result.to_csv('svm_featureimp_final_run_'+now()+'.csv', sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_model(train,test,train_labels,test_labels):\n",
    "    #train model\n",
    "    model = RandomForestClassifier(n_estimators=29, max_features = 0.7999999999999999, n_jobs=-1, verbose = 1, max_depth = 10, bootstrap = True)\n",
    "    model.fit(train, train_labels)\n",
    "   \n",
    "    #test model\n",
    "    rf_predictions = model.predict(test)\n",
    "    score = classification_report(rf_predictions, test_labels, output_dict=True)\n",
    "    df_score = pd.DataFrame(score).transpose()\n",
    "    df_score.to_csv('rf_score_final_run_'+now()+'.csv', sep=\";\", index=False)\n",
    "    \n",
    "    #saving TP\n",
    "    RFTest = test.copy()\n",
    "    RFTest['Predicted'] = rf_predictions\n",
    "    RFTest['Actual'] = test_labels\n",
    "    RFTest = RFTest.loc[RFTest['Predicted'] == RFTest['Actual']]\n",
    "    RF_Predictions = data.iloc[list(RFTest.index.values.tolist())]\n",
    "    RF_Predictions.to_csv('rf_TP_final_run_'+now()+'.csv', sep=\";\", index=False)\n",
    "\n",
    "    #postitive target feature importance\n",
    "    rf_feature_result = pd.DataFrame({'feature': train.columns,'importance': model.feature_importances_}).sort_values('importance', ascending = False)\n",
    "    rf_feature_result.to_csv('rf_featureimp_final_run_'+now()+'.csv', sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree with Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dtgb_model(train,test,train_labels,test_labels):\n",
    "    #train model\n",
    "    model = GradientBoostingClassifier(n_estimators=100, max_depth=4, subsample=0.99, learning_rate=0.977)\n",
    "    model.fit(train, train_labels)\n",
    "    \n",
    "    #test model\n",
    "    dtgb_predictions = model.predict(test)\n",
    "    score = classification_report(dtgb_predictions, test_labels, output_dict=True)\n",
    "    df_score = pd.DataFrame(score).transpose()\n",
    "    df_score.to_csv('/Job/run/dtgb_score_final_run_'+now()+'.csv', sep=\";\", index=False)\n",
    "    \n",
    "    #saving TP\n",
    "    DTGBTest = test.copy()\n",
    "    DTGBTest['Predicted'] = dtgb_predictions\n",
    "    DTGBTest['Actual'] = test_labels\n",
    "    DTGBTest = DTGBTest.loc[DTGBTest['Predicted'] == DTGBTest['Actual']]\n",
    "    DTGB_Predictions = data.iloc[list(DTGBTest.index.values.tolist())]\n",
    "    DTGB_Predictions.to_csv('/run/dtgb_final_TP_final_run_'+now()+'.csv', sep=\";\", index=False)\n",
    "\n",
    "    #postitive target feature importance\n",
    "    dtgb_feature_result = pd.DataFrame({'feature': train.columns,'importance': model.feature_importances_}).sort_values('importance', ascending = False)\n",
    "    dtgb_feature_result.to_csv('/run/dtgb_featureimp_final_run_'+now()+'.csv', sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_model(train,test,train_labels,test_labels):\n",
    "    #train model\n",
    "    logmodel = LogisticRegression(solver = 'lbfgs')\n",
    "    logmodel.fit(train, train_labels)\n",
    "    \n",
    "    #test model\n",
    "    predictions = logmodel.predict(test)\n",
    "    score = classification_report(predictions, test_labels, output_dict=True)\n",
    "    df_score = pd.DataFrame(score).transpose()    \n",
    "    df_score.to_csv('/run/logreg_score_final_run_'+now()+'.csv', sep=\";\", index=False)\n",
    "    \n",
    "    #saving TP\n",
    "    LRTest = test.copy()\n",
    "    LRTest['Predicted'] = predictions\n",
    "    LRTest['Actual'] = test_labels\n",
    "    LRTest = LRTest.loc[LRTest['Predicted'] == LRTest['Actual']]\n",
    "    LR_Predictions = data.iloc[list(LRTest.index.values.tolist())]\n",
    "    LR_Predictions.to_csv('/run/logreg_final_TP_final_run_'+now()+'.csv', sep=\";\", index=False)\n",
    "    \n",
    "    #postive target feature importance\n",
    "    logreg_feature_result = pd.DataFrame({'feature': train.columns,'importance': logmodel.coef_[0]}).sort_values('importance', ascending = False)\n",
    "    logreg_feature_result.to_csv('/run/logreg_featureimp_final_run_'+now()+'.csv', sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non = df_copy.loc[df_copy['is_cyber_victim_1'] == 0]\n",
    "df_cyber = df_copy.loc[df_copy['is_cyber_victim_1'] == 1]\n",
    "sample_size = len(df_cyber)\n",
    "loop_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running model mulitple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  29 out of  29 | elapsed:    4.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  29 out of  29 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    #sampling data\n",
    "    data = pd.read_csv(\"stratified_TP_data_2020-01-14 19:06:54.csv\", sep=';')\n",
    "    \n",
    "    #splitting data\n",
    "    labels = np.array(data.pop('is_cyber_victim_1'))\n",
    "    train, test, train_labels, test_labels = train_test_split(data, labels, stratify = labels, test_size = 0.3, random_state = 21)\n",
    "    \n",
    "    #running models\n",
    "    #dtgb_model(train, test, train_labels, test_labels)\n",
    "    #logreg_model(train, test, train_labels, test_labels)\n",
    "    rf_model(train, test, train_labels, test_labels)   \n",
    "    #svm_model(train, test, train_labels, test_labels)\n",
    "    \n",
    "    #merging TP\n",
    "    #frames = [SVM_Predictions, RF_Predictions, LR_Predictions]\n",
    "    #CombinedPredictions = pd.concat(frames, sort=False)\n",
    "    #CombinedPredictions.sort_index(inplace=True)\n",
    "    #CombinedPredictions.to_csv('combined_TP_run_'+now()+'.csv', sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['migr_background_Eerste generatie migratieachtergrond',\n",
       "       'migr_background_Geen migratieachtergond',\n",
       "       'migr_background_Tweede generatie migratieachtergrond',\n",
       "       'sex_Mannen', 'sex_Vrouwen', 'unemploy_benefit_ ',\n",
       "       'unemploy_benefit_Heeft in betreffend jaar minimaal een maand een WW-uitkering ontvangen',\n",
       "       'debts_ ', 'debts_Heeft WSNP traject in betreffend jaar',\n",
       "       'laborer_cath_ ',\n",
       "       'laborer_cath_Behoort tot huishouden zonder waargenomen inkomen',\n",
       "       'laborer_cath_Directeur-grootaandeelhouder',\n",
       "       'laborer_cath_Meewerkend gezinslid',\n",
       "       'laborer_cath_Nog niet schoolgaand/scholier/student met inkomen',\n",
       "       'laborer_cath_Nog niet schoolgaand/scholier/student zonder inkomen',\n",
       "       'laborer_cath_Ontvanger bijstandsuitkering',\n",
       "       'laborer_cath_Ontvanger pensioenuitkering',\n",
       "       'laborer_cath_Ontvanger uitkering sociale voorziening overig',\n",
       "       'laborer_cath_Ontvanger uitkering ziekte/arbeidsongeschiktheid',\n",
       "       'laborer_cath_Ontvanger werkloosheidsuitkering',\n",
       "       'laborer_cath_Overig zonder inkomen',\n",
       "       'laborer_cath_Overige zelfstandige', 'laborer_cath_Werknemer',\n",
       "       'laborer_cath_Zelfstandig ondernemer', 'edu_lvl_18_cath_ ',\n",
       "       'edu_lvl_18_cath_Basisonderwijs gr1-2',\n",
       "       'edu_lvl_18_cath_Basisonderwijs gr3-8', 'edu_lvl_18_cath_Doctor',\n",
       "       'edu_lvl_18_cath_Havo-, vwo-onderbouw',\n",
       "       'edu_lvl_18_cath_Havo-bovenbouw',\n",
       "       'edu_lvl_18_cath_Hbo-associate degree',\n",
       "       'edu_lvl_18_cath_Hbo-bachelor', 'edu_lvl_18_cath_Hbo-master',\n",
       "       'edu_lvl_18_cath_Mbo1', 'edu_lvl_18_cath_Mbo2',\n",
       "       'edu_lvl_18_cath_Mbo3', 'edu_lvl_18_cath_Mbo4',\n",
       "       'edu_lvl_18_cath_Praktijkonderwijs', 'edu_lvl_18_cath_Vmbo-b/k',\n",
       "       'edu_lvl_18_cath_Vmbo-g/t', 'edu_lvl_18_cath_Vwo-bovenbouw',\n",
       "       'edu_lvl_18_cath_Wo-bachelor', 'edu_lvl_18_cath_Wo-master',\n",
       "       'social_benefit_ ',\n",
       "       'social_benefit_Heeft in betreffend jaar minimaal een maand een bijstandsuitkering ontvangen',\n",
       "       'hh_type_ ', 'hh_type_Eenouderhuishouden',\n",
       "       'hh_type_Eenpersoonshuishouden',\n",
       "       'hh_type_Gehuwd paar met kinderen',\n",
       "       'hh_type_Gehuwd paar zonder kinderen',\n",
       "       'hh_type_Institutioneel huishouden',\n",
       "       'hh_type_Niet-gehuwd paar met kinderen',\n",
       "       'hh_type_Niet-gehuwd paar zonder kinderen',\n",
       "       'hh_type_Overig huishouden', 'indiv_income_ groter dan 0%',\n",
       "       'indiv_income_ groter dan 250%', 'indiv_income_ groter dan 500%',\n",
       "       'indiv_income_ groter dan 750%',\n",
       "       'indiv_income_Institutioneel huishouden',\n",
       "       'indiv_income_Particulier huishouden met onbekend inkomen',\n",
       "       'indiv_income_Studentenhuishouden/niet gehele jaar inkomen',\n",
       "       'indiv_income_4_years_ groter dan 0% (4 jaar)',\n",
       "       'indiv_income_4_years_ groter dan 250% (4 jaar)',\n",
       "       'indiv_income_4_years_ groter dan 500% (4 jaar)',\n",
       "       'indiv_income_4_years_ groter dan 750% (4 jaar)',\n",
       "       'indiv_income_4_years_Geen doelpopulatie in minimaal een van de 3 voorgaande jaren',\n",
       "       'origin_land_8_cath_Marokko',\n",
       "       'origin_land_8_cath_Overige niet-westerse landen',\n",
       "       'origin_land_8_cath_Overige westerse landen',\n",
       "       'origin_land_8_cath_Suriname', 'origin_land_8_cath_Turkije',\n",
       "       'origin_land_8_cath_Voormalige Nederlandse Antillen en Aruba',\n",
       "       'age_5_cath_1', 'age_5_cath_2', 'age_5_cath_3', 'age_5_cath_4',\n",
       "       'age_5_cath_5', 'hh_type_5_cath_(On)gehuwd paar met kinderen',\n",
       "       'hh_type_5_cath_(On)gehuwd paar zonder kinderen',\n",
       "       'hh_type_5_cath_Institutioneel of overig huishouden',\n",
       "       'migr_background_3_cath_Niet-westerse migratieachtergrond',\n",
       "       'edu_lvl_3_cath_Hoger onderwijs', 'edu_lvl_3_cath_Lager onderwijs',\n",
       "       'edu_lvl_3_cath_Middelbaar onderwijs',\n",
       "       'hh_std_income_1e tm 25e percentiel',\n",
       "       'hh_std_income_26e tm 50e percentiel',\n",
       "       'hh_std_income_51e tm 75e percentiel',\n",
       "       'hh_std_income_76e tm 100e percentiel',\n",
       "       'hh_std_income_Onbekend of institutioneel huishouden',\n",
       "       'hh_std_4_years_1e tm 50e percentiel',\n",
       "       'hh_std_4_years_51e tm 100e percentiel',\n",
       "       'comp_hh_income_Inkomen minder dan 100% van de lage-inkomensgrens',\n",
       "       'comp_hh_income_Inkomen minimaal lage-inkomensgrens of hoger',\n",
       "       'comp_hh_income_Onbekend of institutioneel of studentenhuishouden',\n",
       "       'comp_hh_income_4_years_Inkomen minder dan 100% van de lage-inkomensgrens',\n",
       "       'comp_hh_income_4_years_Inkomen minimaal lage-inkomensgrens of hoger',\n",
       "       'comp_hh_income_4_years_Onbekend, institutioneel, studentenhuishouden of niet voor alle jaren bekend',\n",
       "       'city_popu_density_Matig stedelijk (OAD 1000 tot 1500)',\n",
       "       'city_popu_density_Niet stedelijk (OAD minder dan 500)',\n",
       "       'city_popu_density_Sterk stedelijk (OAD 1500 tot 2500)',\n",
       "       'city_popu_density_Weinig stedelijk (OAD 500 tot 1000)',\n",
       "       'city_popu_density_Zeer sterk stedelijk (OAD 2500 of meer)'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = pd.read_csv(\"../David/final svm/svm_TP_run_2020-01-14 12:51:27.csv\", sep=';')\n",
    "dtgb = pd.read_csv(\"dtgb_TP_run_2020-01-14 16:33:11.csv\", sep=';')\n",
    "rf = pd.read_csv(\"rf_TP_run_2020-01-14 16:33:20.csv\", sep=';')\n",
    "lr = pd.read_csv(\"logreg_TP_run_2020-01-14 16:33:15.csv\", sep=';')\n",
    "\n",
    "frames = [svm, dtgb, rf, lr]\n",
    "allmodels = pd.concat(frames, sort=False)\n",
    "allmodels.sort_index(inplace=True)\n",
    "allmodels.to_csv('combined_TP_run_'+now()+'.csv', sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no = pd.read_csv(\"first_sample_2020-01-09 11:58:10.csv\", sep=';')\n",
    "df_sample_non = df_no.loc[df_no['is_cyber_victim_1'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "allmodels['is_cyber_victim_1'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102096"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(allmodels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88131"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_sample_non)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-dd59a4dd9f9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#splitting data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'is_cyber_victim_1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m21\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#running models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2119\u001b[0m                      random_state=random_state)\n\u001b[1;32m   2120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2121\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstratify\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2123\u001b[0m     return list(chain.from_iterable((safe_indexing(a, train),\n",
      "\u001b[0;32m/data/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   1712\u001b[0m         \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0minteger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1713\u001b[0m         \"\"\"\n\u001b[0;32m-> 1714\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1715\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 542\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "for i in range(loop_size):\n",
    "    #sampling data\n",
    "    frame2 = [allmodels, df_sample_non.sample(len(CombinedPredictions))]\n",
    "    data2 = pd.concat(frame2, sort=False, ignore_index=True)\n",
    "    data2.to_csv('TP_sample'+now()+'.csv', sep=\";\", index=False)\n",
    "    \n",
    "    #splitting data\n",
    "    labels = np.array(data2.pop('is_cyber_victim_1'))\n",
    "    train, test, train_labels, test_labels = train_test_split(data2, labels, stratify = labels, test_size = 0.3, random_state = 21)\n",
    "    \n",
    "    #running models\n",
    "    logreg_model(train, test, train_labels, test_labels)\n",
    "    rf_model(train, test, train_labels, test_labels)\n",
    "    svm_model(train, test, train_labels, test_labels)\n",
    "    \n",
    "    #mering TP\n",
    "    #frames = [SVM_Predictions, RF_Predictions, LR_Predictions]\n",
    "    #CombinedPredictions = pd.concat(frames, sort=False)\n",
    "    #CombinedPredictions.sort_index(inplace=True)\n",
    "    #CombinedPredictions.to_csv('combined_TP_run_two.csv', sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
